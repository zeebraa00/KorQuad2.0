{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mecab 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "morphs() missing 1 required positional argument: 'phrase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-74605ee9fafe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'아버지가방에들어가신다'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: morphs() missing 1 required positional argument: 'phrase'"
     ]
    }
   ],
   "source": [
    "out = tokenizer.morphs('아버지가방에들어가신다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 윈도우 프로 미만 버전에서는 지원 안 되는 것으로 생각됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1_2. MeCab 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MeCab.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer.parse('아버지가방에들어가신다\\n오늘은 신나는 화요일.') #한 줄씩 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아버지\\tNNG,*,F,아버지,*,*,*,*\\n가\\tJKS,*,F,가,*,*,*,*\\n방\\tNNG,장소,T,방,*,*,*,*\\n에\\tJKB,*,F,에,*,*,*,*\\n들어가\\tVV,*,F,들어가,*,*,*,*\\n신다\\tEP+EC,*,F,신다,Inflect,EP,EC,시/EP/*+ㄴ다/EC/*\\n오늘\\tNNG,*,T,오늘,*,*,*,*\\n은\\tJX,*,T,은,*,*,*,*\\n신나\\tVV,*,F,신나,*,*,*,*\\n는\\tETM,*,T,는,*,*,*,*\\n화요일\\tNNG,*,T,화요일,Compound,*,*,화/NNG/*+요일/NNG/*\\n.\\tSF,*,*,*,*,*,*,*\\nEOS\\n'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아버지 가 방 에 들어가 신다 오늘 은 신나 는 화요일 . EOS\\n'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.replace(',','')\n",
    "con = re.sub('(\\s.+\\n)',' ',out) # 정규표현식으로 변환해서 형태소만 추출\n",
    "con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 데이터 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.utils import to_unicode\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불특정 문자 제거 함수 정의\n",
    "WIKI_REMOVE_CHARS = re.compile(\"'+|(=+.{2,30}=+)|__TOC__|(ファイル:).+|:(en|de|it|fr|es|kr|zh|no|fi):|\\n\", re.UNICODE)\n",
    "WIKI_SPACE_CHARS = re.compile(\"(\\\\s|゙|゚|　)+\", re.UNICODE)\n",
    "EMAIL_PATTERN = re.compile(\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", re.UNICODE)\n",
    "URL_PATTERN = re.compile(\"(ftp|http|https)?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", re.UNICODE)\n",
    "WIKI_REMOVE_TOKEN_CHARS = re.compile(\"(\\\\*$|:$|^파일:.+|^;)\", re.UNICODE)\n",
    "MULTIPLE_SPACES = re.compile(' +', re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 training data 취합\n",
    "\n",
    "tokenizer = MeCab.Tagger()\n",
    "\n",
    "output_fname = \"../Data/output/processed_korquad_train_fianl.txt\"\n",
    "output_file = open(output_fname, 'w', encoding ='utf-8')\n",
    "\n",
    "for i in range(39):\n",
    "    corpus_fname = \"../Data/input/korquad2.1_train_\" + str(i) + \".json\"\n",
    "    \n",
    "    with open(corpus_fname) as f1:\n",
    "        dataset_json = json.load(f1)\n",
    "        dataset = dataset_json['data']\n",
    "        \n",
    "        for article in dataset:\n",
    "            w_lines = []\n",
    "            paragraph = article['context']\n",
    "            \n",
    "            # 불필요한 단어,문자 제거\n",
    "            paragraph = re.sub('(\\n+)','\\n', paragraph) # \\n이 두번이상 나오는 부분 한번으로 줄임\n",
    "            paragraph = re.sub(EMAIL_PATTERN, ' ', paragraph)  # remove email pattern\n",
    "            paragraph = re.sub(URL_PATTERN, ' ', paragraph) # remove url pattern\n",
    "            #paragraph = re.sub(WIKI_REMOVE_CHARS, ' ', paragraph)  # remove unnecessary chars\n",
    "            paragraph = re.sub(WIKI_SPACE_CHARS, ' ', paragraph) \n",
    "            paragraph = re.sub(MULTIPLE_SPACES, ' ', paragraph)\n",
    "            \n",
    "            # 형태소 분석 (이 과정에서 중간의 \\n 들은 다 제거됨)\n",
    "            paragraph = tokenizer.parse(paragraph)\n",
    "            paragraph = paragraph.replace(',','')\n",
    "            paragraph = re.sub('(\\s.+\\n)',' ', paragraph) # 정규표현식으로 변환해서 형태소만 추출\n",
    "            paragraph.replace('\\n','')  # 마지막 \\n 제거\n",
    "            w_lines.append(paragraph)\n",
    "            \n",
    "            for qa in article['qas']:\n",
    "                q_text = qa['question']\n",
    "                a_text = qa['answer']['text']\n",
    "                qas = q_text + \" \" + a_text\n",
    "                \n",
    "                # 형태소 분석\n",
    "                qas = tokenizer.parse(qas)\n",
    "                qas = qas.replace(',','')\n",
    "                qas = re.sub('(\\s.+\\n)',' ', qas) # 정규표현식으로 변환해서 형태소만 추출\n",
    "                w_lines.append(qas)\n",
    "                \n",
    "            for line in w_lines:\n",
    "                output_file.writelines(line + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 dev data 취합\n",
    "output_fname = \"../Data/output/processed_korquad_dev_all.txt\"\n",
    "output_file = open(output_fname, 'w', encoding ='utf-8')\n",
    "\n",
    "for i in range(5):\n",
    "    corpus_fname = \"../Data/input/korquad2.1_dev_\" + str(i) + \".json\"\n",
    "    \n",
    "    with open(corpus_fname) as f1:\n",
    "        dataset_json = json.load(f1)\n",
    "        dataset = dataset_json['data']\n",
    "        \n",
    "        for article in dataset:\n",
    "            w_lines = []\n",
    "            paragraph = article['context']\n",
    "            \n",
    "            # 불필요한 단어,문자 제거\n",
    "            paragraph = re.sub('(\\n+)','\\n', paragraph) # \\n이 두번이상 나오는 부분 한번으로 줄임\n",
    "            paragraph = re.sub(EMAIL_PATTERN, ' ', paragraph)  # remove email pattern\n",
    "            paragraph = re.sub(URL_PATTERN, ' ', paragraph) # remove url pattern\n",
    "            #paragraph = re.sub(WIKI_REMOVE_CHARS, ' ', paragraph)  # remove unnecessary chars\n",
    "            paragraph = re.sub(WIKI_SPACE_CHARS, ' ', paragraph) \n",
    "            paragraph = re.sub(MULTIPLE_SPACES, ' ', paragraph)\n",
    "            \n",
    "            # 형태소 분석 (이 과정에서 중간의 \\n 들은 다 제거됨)\n",
    "            paragraph = tokenizer.parse(paragraph)\n",
    "            paragraph = paragraph.replace(',','')\n",
    "            paragraph = re.sub('(\\s.+\\n)',' ', paragraph) # 정규표현식으로 변환해서 형태소만 추출\n",
    "            paragraph.replace('\\n','') # 마지막 \\n 제거\n",
    "            w_lines.append(paragraph)\n",
    "            \n",
    "            for qa in article['qas']:\n",
    "                q_text = qa['question']\n",
    "                a_text = qa['answer']['text']\n",
    "                qas = q_text + \" \" + a_text\n",
    "                \n",
    "                # 형태소 분석\n",
    "                qas = tokenizer.parse(qas)\n",
    "                qas = qas.replace(',','')\n",
    "                qas = re.sub('(\\s.+\\n)',' ', qas) # 정규표현식으로 변환해서 형태소만 추출\n",
    "                w_lines.append(qas)\n",
    "                \n",
    "            for line in w_lines:\n",
    "                output_file.writelines(line + '\\n')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
